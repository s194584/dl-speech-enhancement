{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for project results in 02456 Deep Learning\n",
    "This notebook will load our final model and will perform inference on 6 randomly selected clean-noise pairs from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, the model and audio files will be downloaded from Google Drive. Note that these will be stored locally in the folder \"notebook_files\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T09:56:02.544520Z",
     "start_time": "2023-12-21T09:56:02.505373Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.autoencoder_without_PQC.AudioDec import Generator as GeneratorAudioDec\n",
    "from models.autoencoder.AudioDec import Generator as generator_audiodec_original\n",
    "from models.vocoder.HiFiGAN import Generator as generator_hifigan\n",
    "import torchaudio\n",
    "import glob\n",
    "from dataloader.data_utils import add_noise\n",
    "import os\n",
    "# from bin.utils import load_config\n",
    "import yaml\n",
    "\n",
    "directory = \"notebook_files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T09:56:02.720811Z",
     "start_time": "2023-12-21T09:56:02.524712Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_config(path_to_config):\n",
    "    with open(path_to_config, \"r\") as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    return config\n",
    "\n",
    "def define_AD_model(directory):\n",
    "    # path_to_model_dir = os.path.join(\"exp\",\"denoise\",\"symAD_vctk_48000_hop300\",\"config.yml\")\n",
    "    # Encoder\n",
    "\n",
    "    print(\"encoder\")\n",
    "    path_to_config_encoder = os.path.join(\n",
    "        directory, \"config\", \"denoise\", \"symAD_vctk_48000_hop300.yaml\"\n",
    "    )\n",
    "    path_to_model_encoder = os.path.join(directory, \"audiodec_encoder.pkl\")\n",
    "    encoder_config = load_config(path_to_config_encoder)\n",
    "    encoder = generator_audiodec_original(**encoder_config[\"generator_params\"])\n",
    "    encoder.load_state_dict(\n",
    "        torch.load(path_to_model_encoder, map_location=\"cpu\")[\"model\"][\"generator\"]\n",
    "    )\n",
    "    print(\"decoder\")\n",
    "    # Decoder\n",
    "    path_to_config_decoder = os.path.join(\n",
    "        directory, \"config\", \"vocoder\", \"AudioDec_v1_symAD_vctk_48000_hop300_clean.yaml\"\n",
    "    )\n",
    "    path_to_model_decoder = os.path.join(directory, \"audiodec_vocoder.pkl\")\n",
    "    decoder_config = load_config(path_to_config_decoder)\n",
    "    decoder = generator_hifigan(**decoder_config[\"generator_params\"])\n",
    "    decoder.load_state_dict(\n",
    "        torch.load(path_to_model_decoder, map_location=\"cpu\")[\"model\"][\"generator\"]\n",
    "    )\n",
    "\n",
    "    def forward(x):\n",
    "        x = encoder.encoder(x)\n",
    "        x = encoder.projector(x)\n",
    "        x, _, _ = encoder.quantizer(x)\n",
    "        return decoder(x)\n",
    "\n",
    "    return forward\n",
    "\n",
    "config = {\n",
    "    \"input_channels\": 1,\n",
    "    \"output_channels\": 1,\n",
    "    \"encode_channels\": 32,\n",
    "    \"decode_channels\": 32,\n",
    "    \"bias\": True,\n",
    "    \"enc_ratios\": [2, 4, 8, 16],\n",
    "    \"dec_ratios\": [16, 8, 4, 2],\n",
    "    \"enc_strides\": [3, 4, 5, 5],\n",
    "    \"dec_strides\": [5, 5, 4, 3],\n",
    "    \"mode\": 'causal'\n",
    "}\n",
    "\n",
    "model: dict[str, torch.nn.Module] = {}\n",
    "model[\"generator\"] = GeneratorAudioDec(**config)\n",
    "state_dict = torch.load(directory+\"model.pkl\", map_location=\"cpu\")\n",
    "model[\"generator\"].load_state_dict(state_dict)\n",
    "AD_model = define_AD_model(directory)\n",
    "\n",
    "SAMPLE_RATE = 24000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T09:56:02.832688Z",
     "start_time": "2023-12-21T09:56:02.732436Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_file_paths(prefix):\n",
    "    return glob.glob(directory+prefix+\"*\"+\".wav\")\n",
    "clean_file_paths = get_file_paths(\"clean\")\n",
    "noise_file_paths = get_file_paths(\"noise\")\n",
    "\n",
    "def load_samples(file_paths):\n",
    "    samples = []\n",
    "    for file_path in file_paths:\n",
    "        audio, original_sample_rate = torchaudio.load(file_path)\n",
    "        # Down sample to fit trained model\n",
    "        audio = torchaudio.functional.resample(audio, original_sample_rate, SAMPLE_RATE)\n",
    "        samples.append(audio)\n",
    "    return  samples\n",
    "\n",
    "\n",
    "clean_samples = load_samples(clean_file_paths)\n",
    "noise_samples = load_samples(noise_file_paths)\n",
    "\n",
    "for i, (clean_sample,noise_sample) in enumerate(zip(clean_samples,noise_samples)):\n",
    "    if clean_sample.shape[1] > noise_sample.shape[1]:\n",
    "        clean_samples[i] = clean_sample[:, :noise_sample.shape[1]]\n",
    "    else:\n",
    "        noise_samples[i] = noise_sample[:, :clean_sample.shape[1]]\n",
    "\n",
    "\n",
    "# Storing trimmed noise files\n",
    "for i, noise_sample in enumerate(noise_samples):\n",
    "    # (C,L) -> (B,C,L)\n",
    "    file_path = directory+f\"noise{i+1}.wav\"\n",
    "    torchaudio.save(file_path,noise_sample,SAMPLE_RATE)\n",
    "    noise_file_paths[i]=file_path\n",
    "\n",
    "# Mixing at SNR of 15\n",
    "mixed_samples = [add_noise(speech,noise,torch.tensor(15)) for speech,noise in zip(clean_samples,noise_samples)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T09:56:07.681295Z",
     "start_time": "2023-12-21T09:56:02.834684Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "predictions_AD = []\n",
    "pred_file_paths = []\n",
    "pred_AD_file_paths = []\n",
    "mixed_file_paths = []\n",
    "with torch.no_grad():\n",
    "    for i, mixed_sample in enumerate(mixed_samples):\n",
    "        print(f\"Sample {i}\")\n",
    "        # (C,L) -> (B,C,L)\n",
    "        file_path = directory+f\"mixed{i+1}.wav\"\n",
    "        torchaudio.save(file_path,mixed_sample,SAMPLE_RATE)\n",
    "        mixed_file_paths.append(file_path)\n",
    "    \n",
    "    \n",
    "        mixed_sample = mixed_sample.unsqueeze(0)\n",
    "        pred = model[\"generator\"](mixed_sample)\n",
    "        pred = pred.squeeze(0).detach()\n",
    "        \n",
    "        pred_AD = torch.squeeze(AD_model(mixed_sample), dim=0)\n",
    "    \n",
    "        predictions.append(pred)\n",
    "        predictions_AD.append(pred_AD)\n",
    "        \n",
    "        file_path = directory+f\"prediction{i+1}.wav\"\n",
    "        torchaudio.save(file_path,pred,SAMPLE_RATE)\n",
    "        pred_file_paths.append(file_path)\n",
    "        \n",
    "        file_path = directory+f\"prediction{i+1}_AD.wav\"\n",
    "        torchaudio.save(file_path, pred_AD, SAMPLE_RATE)\n",
    "        pred_AD_file_paths.append(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showcase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T09:56:13.746218Z",
     "start_time": "2023-12-21T09:56:07.683293Z"
    }
   },
   "outputs": [],
   "source": [
    "import IPython,time\n",
    "from IPython.display import Audio, HTML, Markdown, display\n",
    "\n",
    "\n",
    "def html_label_audio(audio_path):\n",
    "    # print(audio_path)\n",
    "    sep = \"notebook_files\\\\\"\n",
    "    return f\"\"\"\n",
    "    <div>\n",
    "        <p>{audio_path.split(sep)[1]}</p>\n",
    "        <audio controls>\n",
    "            <source src=\"{audio_path}\" type=\"audio/wav\">\n",
    "        </audio>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "def html_wrap_cells(cells):\n",
    "    connector = \"\\n\"\n",
    "    return f\"\"\"\n",
    "    <h2>Experiment</h2>\n",
    "    <h4>Input</h4>\n",
    "    <div style=\"display: flex;flex:50%; align-items: center; justify-content: space-around;\">\n",
    "        {cells[0]}\n",
    "        {cells[1]}\n",
    "    </div>\n",
    "    <h4>Inference</h4>\n",
    "    <div style=\"display: flex;flex:50%; align-items: center; justify-content: space-around;\">\n",
    "        {cells[2]}\n",
    "        {cells[3]}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "for i, sample in enumerate(clean_samples):\n",
    "    clean_html = html_label_audio(clean_file_paths[i])\n",
    "    noise_html = html_label_audio(noise_file_paths[i])\n",
    "    mixed_html = html_label_audio(mixed_file_paths[i])\n",
    "    pred_html = html_label_audio(pred_file_paths[i])\n",
    "    pred_AD_html = html_label_audio(pred_AD_file_paths[i])\n",
    "\n",
    "    display(HTML(data=html_wrap_cells([clean_html,noise_html,mixed_html,pred_html,pred_AD_html])))\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T09:56:13.762006Z",
     "start_time": "2023-12-21T09:56:13.745177Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
